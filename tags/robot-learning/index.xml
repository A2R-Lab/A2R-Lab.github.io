<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robot Learning on A²R Lab</title>
    <link>https://a2r-lab.github.io/tags/robot-learning/</link>
    <description>Recent content in Robot Learning on A²R Lab</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Brian Plancher</copyright>
    <lastBuildDate>Fri, 14 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://a2r-lab.github.io/tags/robot-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tiny Robot Learning: Expanding Access to Edge ML as a Step Toward Accessible Robotics</title>
      <link>https://a2r-lab.github.io/publication/tinyrobotlearningrssaccessibility/</link>
      <pubDate>Fri, 14 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://a2r-lab.github.io/publication/tinyrobotlearningrssaccessibility/</guid>
      <description>The high barriers to entry associated with robotics, in particular its high cost, has rendered it inaccessibility for many. In this poster we present our early efforts to begin to address these challenges through edge machine learning (ML). We show how ultra-low-cost robot and computational hardware paired with open-source software and courseware can be leveraged for hands-on education globally and the beginnings of a globally diverse research community.</description>
    </item>
    
    <item>
      <title>Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion</title>
      <link>https://a2r-lab.github.io/publication/justround/</link>
      <pubDate>Mon, 29 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://a2r-lab.github.io/publication/justround/</guid>
      <description>Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as 4.2x without impacting learning performance.</description>
    </item>
    
    <item>
      <title>Tiny Robot Learning: Challenges and Directions for Machine Learning in Resource-Constrained Robots</title>
      <link>https://a2r-lab.github.io/publication/tinyrobotlearning/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://a2r-lab.github.io/publication/tinyrobotlearning/</guid>
      <description>Tiny robot learning lies at the intersection of embedded systems, robotics, and ML, compounding the challenges of these domains. This paper gives a brief survey of the tiny robot learning space, elaborates on key challenges, and proposes promising opportunities for future work in ML system design.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning to Enable Robust Robotic Model Predictive Control</title>
      <link>https://a2r-lab.github.io/publication/levthesis/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://a2r-lab.github.io/publication/levthesis/</guid>
      <description>This thesis proposes a novel algorithm, “Reference-Guided, Value-Based MPC,” which combines model predictive control (MPC) and reinforcement learning (RL) to compute feasible trajectories for a robotic arm. The algorithm does this while 1) achieving an almost 50% higher planning success rate than standard MPC, 2) solving in sparse environments considered unsolvable by current state of the art algorithms, and 3) generalizing its solutions to different environment initializations.</description>
    </item>
    
  </channel>
</rss>
